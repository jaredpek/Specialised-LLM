{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation for LLM Fine-Tuning (Gemini 2.5 Flash with PDF Chunks)\n",
    "\n",
    "This notebook provides a two-stage workflow for generating synthetic data. First, it preprocesses large PDF files from a `raw_data` directory by splitting them into smaller chunks and copying the original file into a `chunked_data` directory. Second, it iterates through all files in `chunked_data` (both chunks and full files), uploading each one to the `gemini-2.5-flash` model to generate synthetic input-output pairs.\n",
    "\n",
    "### Workflow:\n",
    "1.  **Setup**: Install and import necessary libraries.\n",
    "2.  **Environment Configuration**: Load your Gemini API key from environment variables.\n",
    "3.  **API and Path Configuration**: Configure directories, the Gemini client, and model parameters.\n",
    "4.  **Preprocessing**: A dedicated function splits source PDFs and copies the original files into the `chunked_data` directory.\n",
    "5.  **Data Generation**: The script iterates through the pre-made PDF chunks and the full PDFs, generating a consistent number of pairs for each.\n",
    "6.  **Save Output**: All generated data is grouped by the original source file and saved to CSV files in the `generated_data` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import shutil\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "import pypdf\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model and Path Configuration\n",
    "\n",
    "We define our parameters, including the model ID, directory paths, and the chunking settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = os.getenv('GEMINI_MODEL_ID')\n",
    "\n",
    "GEMINI_TOKENS = json.loads(os.getenv('GEMINI_TOKENS', []))\n",
    "MODELS = [genai.Client(api_key=token) for token in GEMINI_TOKENS]\n",
    "\n",
    "RAW_DATA_DIR = os.getenv('RAW_DATA_DIR')\n",
    "CHUNKED_DATA_DIR = os.getenv('CHUNKED_DATA_DIR')\n",
    "GENERATED_DATA_DIR = os.getenv('GENERATED_DATA_DIR')\n",
    "\n",
    "CHUNK_SIZE = int(os.getenv('CHUNK_SIZE'))\n",
    "CHUNK_OVERLAP = int(os.getenv('CHUNK_OVERLAP'))\n",
    "\n",
    "MAX_CONCURRENCY = int(os.getenv('MAX_CONCURRENCY'))\n",
    "PAIRS_PER_ITERATION = int(os.getenv('PAIRS_PER_ITERATION'))\n",
    "NUM_ITERATIONS = int(os.getenv('NUM_ITERATIONS'))\n",
    "NUM_RETRIES = int(os.getenv('NUM_RETRIES'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing Step: Splitting and Copying PDFs\n",
    "\n",
    "This function reads all PDFs from `raw_data`, saves smaller chunks to `chunked_data`, and also copies the original full PDF to `chunked_data`. Run this cell once to prepare your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_chunk_pdfs(source_dir=RAW_DATA_DIR,\n",
    "                              dest_dir=CHUNKED_DATA_DIR,\n",
    "                              chunk_size=CHUNK_SIZE,\n",
    "                              overlap=CHUNK_OVERLAP,\n",
    "                              stage_key='PREPROCESSING'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Reads all PDFs from a source directory, splits them into chunks,\n",
    "    saves chunks to a destination directory, and copies the original file.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)\n",
    "        \n",
    "    source_pdfs = glob.glob(os.path.join(source_dir, \"*.pdf\"))\n",
    "    print(f\"[{stage_key}] Found {len(source_pdfs)} PDFs in '{source_dir}'\\n\")\n",
    "\n",
    "    for pdf_path in source_pdfs:\n",
    "        original_basename = os.path.basename(pdf_path).replace('.pdf', '')\n",
    "        try:\n",
    "            print(f\"[{stage_key}] Starting to chunk {os.path.basename(pdf_path)}\")\n",
    "            pdf_reader = pypdf.PdfReader(pdf_path)\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            step = chunk_size - overlap\n",
    "            \n",
    "            for i in range(0, num_pages, step):\n",
    "                pdf_writer = pypdf.PdfWriter()\n",
    "                end_page = min(i + chunk_size, num_pages)\n",
    "                \n",
    "                for page_num in range(i, end_page):\n",
    "                    pdf_writer.add_page(pdf_reader.pages[page_num])\n",
    "                \n",
    "                chunk_filename = f\"{original_basename}_chunk_{(i // step) + 1}.pdf\"\n",
    "                chunk_path = os.path.join(dest_dir, chunk_filename)\n",
    "                \n",
    "                with open(chunk_path, 'wb') as out_pdf:\n",
    "                    pdf_writer.write(out_pdf)\n",
    "            print(f\"[{stage_key}] Successfully chunked {os.path.basename(pdf_path)}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{stage_key}] Error chunking {pdf_path} due to {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_chunk_pdfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Generation Step\n",
    "\n",
    "This is the main generation loop. It iterates through all files in `chunked_data` (both chunks and full documents), generates pairs for each, and saves the combined results into CSV files in the `generated_data` directory, grouped by the original filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemini-2.5-flash-lite'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_GENERATION_PROMPT = r\"\"\"\n",
    "You are an expert data scientist tasked with creating a high-quality dataset for instruction-tuning a large language model.\n",
    "\n",
    "Your primary goal is to generate {0} distinct and high-quality instruction-response pairs from the provided document.\n",
    "\n",
    "## Key Instructions\n",
    "1.  **Instruction (`input`):** Create a clear and specific {1} a user would provide. Some examples of these {1} include: {2}\n",
    "2.  **Clear Definition:** Ensure that relevant and key terms included in the response are clearly defined in the response.\n",
    "3.  **Response (`output`):** Write a detailed, accurate, and direct answer as if you are an expert on the subject. The response must fully and exclusively satisfy the user's instruction, and respond in a complete and factual manner. Make sure to define all the required technical terms clearly and that the response fully answers the question\n",
    "4.  **Source Grounding:** Base all responses **exclusively** on the information within the provided document.\n",
    "5.  **No Self-Reference:** Do not mention the source document in your responses. Avoid any phrases like \"According to the document...\" or \"The provided text states...\".\n",
    "\n",
    "## Formatting Requirements\n",
    "* Your entire output **must** be a single, valid JSON object. Do not include any text, explanations, or markdown formatting before or after the JSON structure.\n",
    "* **CRITICAL:** All strings within the JSON must be properly escaped to ensure the output is parsable.\n",
    "    * All backslashes (`\\`) must be escaped as (`\\\\`). For example, the text `\\x48` must be written in the JSON string as `\\\\x48`, and `\\n` must be written as `\\\\n`.\n",
    "    * All newlines (`\\n`) must be escaped as (`\\\\n`). For example, the text `hi\\n` must be written in the JSON string as `hi\\\\n`.\n",
    "    * All double quotes (`\"`) must be escaped as (`\\\"`). For example, the text `he said \"hello\"` must be written as `he said \\\"hello\\\"` and `\"` must be written as `\\\"`.\n",
    "    * Do not escape any other characters as those are invalid characters. For example, the text `this 'item'` must be written as `this 'item'` and `printf(\"%d\\n\", 5);` must be written as `printf(\\\"%d\\\\n\\\", 5);`.\n",
    "\n",
    "Use the following JSON structure:\n",
    "```json\n",
    "{{\n",
    "  \"results\": [\n",
    "    {{\n",
    "      \"input\": \"Your first generated input here.\",\n",
    "      \"output\": \"Your first generated output here.\"\n",
    "    }},\n",
    "    {{\n",
    "      \"input\": \"Your second generated input here.\",\n",
    "      \"output\": \"Your second generated output here.\"\n",
    "    }},\n",
    "    ... and so on for all {0} pairs. \n",
    "  ]\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TYPES = [\n",
    "    'question',\n",
    "    'statement',\n",
    "    'summary',\n",
    "    'comparison',\n",
    "    'analysis',\n",
    "]\n",
    "TEMPERATURE = float(os.getenv(\"TEMPERATURE\"))\n",
    "TOP_P = float(os.getenv(\"TOP_P\"))\n",
    "WAIT_BETWEEN_REQUESTS = int(os.getenv(\"WAIT_BETWEEN_REQUESTS\"))\n",
    "\n",
    "def get_input_type(input_type):\n",
    "    concept_name = '<CONCEPT NAME>'\n",
    "    alt_concept_name = '<ALT CONCEPT NAME'\n",
    "    mapping = {\n",
    "        'question': [\n",
    "            f'What is this {concept_name} about?',\n",
    "            f'What is the definition of {concept_name}?', f'How is {concept_name} implemented?',\n",
    "        ],\n",
    "        'statement': [\n",
    "            f'Define the concept of {concept_name}.',\n",
    "            f'Provide a breakdown of all the methods of implementation of {concept_name}.',\n",
    "        ],\n",
    "        'summary': [\n",
    "            f'Provide a comprehensive summary of {concept_name}.',\n",
    "            f'Summarise the benefits and challenges of implementing {concept_name}.',\n",
    "            f'Can you summarise the key considerations behind {concept_name}?',\n",
    "        ],\n",
    "        'comparison': [\n",
    "            f'Compare {concept_name} with {alt_concept_name}.',\n",
    "            f'What are the differences between {concept_name} and {alt_concept_name}?',\n",
    "        ],\n",
    "        'analysis': [\n",
    "            f'Analyse {concept_name} with respect to <ANALYSIS METRIC>.',\n",
    "            f'What insights can you derive from the analysis of {concept_name}?',\n",
    "        ],\n",
    "    }\n",
    "    return mapping.get(input_type) or mapping['question']\n",
    "\n",
    "def get_base_name(file_name):\n",
    "    return re.sub(r'\\.[^.]+$', '', os.path.basename(file_name))\n",
    "\n",
    "def generate_data_for_chunk(file_path,\n",
    "                            model,\n",
    "                            model_id=MODEL_ID,\n",
    "                            dest_dir=GENERATED_DATA_DIR,\n",
    "                            data_generation_prompt=DATA_GENERATION_PROMPT,\n",
    "                            iterations=NUM_ITERATIONS,\n",
    "                            pairs_per_iteration=PAIRS_PER_ITERATION,\n",
    "                            retries=NUM_RETRIES,\n",
    "                            stage_key='DATA_GENERATION',\n",
    "                            input_types=INPUT_TYPES,\n",
    "                            temperature=TEMPERATURE,\n",
    "                            top_p=TOP_P):\n",
    "\n",
    "    full_file_name = os.path.basename(file_path)\n",
    "    base_file_name = get_base_name(full_file_name)\n",
    "    trgt_dir = os.path.join(dest_dir, base_file_name)\n",
    "    \n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)\n",
    "    if not os.path.exists(trgt_dir):\n",
    "        os.makedirs(trgt_dir)\n",
    "    \n",
    "    try:\n",
    "        uploaded_file = model.files.upload(file=file_path, config={'display_name': full_file_name})\n",
    "        full_df = pd.DataFrame()\n",
    "\n",
    "        while model.files.get(name=uploaded_file.name).state != \"ACTIVE\":\n",
    "            print(f\"[{stage_key}] Uploading file '{full_file_name}'\")\n",
    "            time.sleep(5)\n",
    "\n",
    "        print(f\"[{stage_key}] Successfully uploaded '{full_file_name}'\")\n",
    "        \n",
    "        for input_type in input_types:\n",
    "            formatted_data_generation_prompt = data_generation_prompt.format(pairs_per_iteration, input_type, ', '.join(get_input_type(input_type)))\n",
    "            for iteration_num in range(1, iterations + 1):\n",
    "                print(f\"[{stage_key}] Starting data generation of {input_type} ({iteration_num}/{iterations}) for '{full_file_name}'\")\n",
    "                for attempt in range(retries):\n",
    "                    try:\n",
    "                        response = model.models.generate_content(\n",
    "                            model=model_id,\n",
    "                            contents=[formatted_data_generation_prompt, uploaded_file],\n",
    "                            config={\"temperature\": temperature, \"top_p\": top_p}\n",
    "                        )\n",
    "                        response_text = response.text[7:-3]\n",
    "                        results = json.loads(response_text)[\"results\"]\n",
    "                        print(f\"[{stage_key}] Received {len(results)} pairs in response for {input_type} generation ({iteration_num}/{iterations}) for '{full_file_name}'{' (attempt' + attempt + '/' + retries + ')' if attempt > 1 else ''}\")\n",
    "\n",
    "                        generated_pairs = [{\n",
    "                            \"input\": r\"{}\".format(result[\"input\"].strip()),\n",
    "                            \"output\": r\"{}\".format(result[\"output\"].strip())\n",
    "                        } for result in results if result.get('input') and result.get('output')]\n",
    "                        print(f\"[{stage_key}] Cumulative total of {len(generated_pairs)} pairs generated for '{full_file_name}' after generation of {input_type} ({iteration_num}/{iterations}){' (attempt' + attempt + '/' + retries + ')' if attempt > 1 else ''}\")\n",
    "\n",
    "                        df = pd.DataFrame(generated_pairs)\n",
    "                        pd.concat([full_df, df])\n",
    "                        df.to_csv(os.path.join(trgt_dir, f'{base_file_name}_{input_type}_{iteration_num}.csv'), index=False)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"[{stage_key}] Data generation ({iteration_num}/{iterations}){' (attempt' + attempt + '/' + retries + ')' if attempt > 1 else ''} for '{full_file_name}' failed due to {e}\")\n",
    "                        time.sleep(5)\n",
    "                time.sleep(5)\n",
    "        return full_df\n",
    "    except Exception as e:\n",
    "        print(f\"[{stage_key}] Data generation for '{full_file_name}' failed due to {e}\")\n",
    "\n",
    "    finally:\n",
    "        if uploaded_file:\n",
    "            model.files.delete(name=uploaded_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_for_chunks(file_paths=[],\n",
    "                             raw_dir=RAW_DATA_DIR,\n",
    "                             models=MODELS,\n",
    "                             chunked_dir=CHUNKED_DATA_DIR,\n",
    "                             dest_dir=GENERATED_DATA_DIR,\n",
    "                             stage_key='DATA_GENERATION',\n",
    "                             max_workers=MAX_CONCURRENCY):\n",
    "    \n",
    "    original_pdf_files = glob.glob(os.path.join(raw_dir, \"*.pdf\")) if not len(file_paths) else file_paths\n",
    "\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)\n",
    "\n",
    "    print(f\"[{stage_key}] Found {len(original_pdf_files)} original documents to process for generation\")\n",
    "    print(f\"[{stage_key}] Using {max_workers} workers for data generation, at {math.ceil(max_workers / len(models))} requests per model\\n\")\n",
    "\n",
    "    all_files_to_process = []\n",
    "    \n",
    "    for original_pdf_file in original_pdf_files:\n",
    "        base_name = get_base_name(original_pdf_file)\n",
    "        files_to_process = sorted(glob.glob(os.path.join(chunked_dir, f\"{base_name}*.pdf\")))\n",
    "\n",
    "        if not files_to_process:\n",
    "            print(f\"[{stage_key}] No files found for '{base_name}'\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"[{stage_key}] Generating data for '{base_name}'\")\n",
    "        all_files_to_process.extend(files_to_process)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(generate_data_for_chunk, file_path, models[i % len(models)]): file_path \n",
    "            for i, file_path in enumerate(all_files_to_process)\n",
    "        }\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            file_path = futures[future]\n",
    "            incremental_df = future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_data_for_chunks()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
