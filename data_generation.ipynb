{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation for LLM Fine-Tuning (Gemini 2.5 Flash with PDF Chunks)\n",
    "\n",
    "This notebook provides a two-stage workflow for generating synthetic data. First, it preprocesses large PDF files from a `raw_data` directory by splitting them into smaller chunks and copying the original file into a `chunked_data` directory. Second, it iterates through all files in `chunked_data` (both chunks and full files), uploading each one to the `gemini-2.5-flash` model to generate synthetic input-output pairs.\n",
    "\n",
    "### Workflow:\n",
    "1.  **Setup**: Install and import necessary libraries.\n",
    "2.  **Environment Configuration**: Load your Gemini API key from environment variables.\n",
    "3.  **API and Path Configuration**: Configure directories, the Gemini client, and model parameters.\n",
    "4.  **Preprocessing**: A dedicated function splits source PDFs and copies the original files into the `chunked_data` directory.\n",
    "5.  **Data Generation**: The script iterates through the pre-made PDF chunks and the full PDFs, generating a consistent number of pairs for each.\n",
    "6.  **Save Output**: All generated data is grouped by the original source file and saved to CSV files in the `generated_data` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import time\n",
    "import shutil\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "import pypdf\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model and Path Configuration\n",
    "\n",
    "We define our parameters, including the model ID, directory paths, and the chunking settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = os.getenv('MODEL_ID')\n",
    "MODEL = genai.Client(api_key=os.getenv(\"GEMINI_TOKEN\"))\n",
    "RAW_DATA_DIR = os.getenv('RAW_DATA_DIR')\n",
    "CHUNKED_DATA_DIR = os.getenv('CHUNKED_DATA_DIR')\n",
    "GENERATED_DATA_DIR = os.getenv('GENERATED_DATA_DIR')\n",
    "\n",
    "CHUNK_SIZE = int(os.getenv('CHUNK_SIZE'))\n",
    "CHUNK_OVERLAP = int(os.getenv('CHUNK_OVERLAP'))\n",
    "\n",
    "MAX_CONCURRENCY = int(os.getenv('MAX_CONCURRENCY'))\n",
    "PAIRS_PER_ITERATION = int(os.getenv('PAIRS_PER_ITERATION'))\n",
    "NUM_ITERATIONS = int(os.getenv('NUM_ITERATIONS'))\n",
    "NUM_RETRIES = int(os.getenv('NUM_RETRIES'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing Step: Splitting and Copying PDFs\n",
    "\n",
    "This function reads all PDFs from `raw_data`, saves smaller chunks to `chunked_data`, and also copies the original full PDF to `chunked_data`. Run this cell once to prepare your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_chunk_pdfs(source_dir=RAW_DATA_DIR,\n",
    "                              dest_dir=CHUNKED_DATA_DIR,\n",
    "                              chunk_size=CHUNK_SIZE,\n",
    "                              overlap=CHUNK_OVERLAP,\n",
    "                              stage_key='PREPROCESSING'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Reads all PDFs from a source directory, splits them into chunks,\n",
    "    saves chunks to a destination directory, and copies the original file.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)\n",
    "        \n",
    "    source_pdfs = glob.glob(os.path.join(source_dir, \"*.pdf\"))\n",
    "    print(f\"[{stage_key}] Found {len(source_pdfs)} PDFs in '{source_dir}'\\n\")\n",
    "\n",
    "    for pdf_path in source_pdfs:\n",
    "        original_basename = os.path.basename(pdf_path).replace('.pdf', '')\n",
    "        try:\n",
    "            print(f\"[{stage_key}] Starting to chunk {os.path.basename(pdf_path)}\")\n",
    "            pdf_reader = pypdf.PdfReader(pdf_path)\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            step = chunk_size - overlap\n",
    "            \n",
    "            for i in range(0, num_pages, step):\n",
    "                pdf_writer = pypdf.PdfWriter()\n",
    "                end_page = min(i + chunk_size, num_pages)\n",
    "                \n",
    "                for page_num in range(i, end_page):\n",
    "                    pdf_writer.add_page(pdf_reader.pages[page_num])\n",
    "                \n",
    "                chunk_filename = f\"{original_basename}_chunk_{(i // step) + 1}.pdf\"\n",
    "                chunk_path = os.path.join(dest_dir, chunk_filename)\n",
    "                \n",
    "                with open(chunk_path, 'wb') as out_pdf:\n",
    "                    pdf_writer.write(out_pdf)\n",
    "            print(f\"[{stage_key}] Successfully chunked {os.path.basename(pdf_path)}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{stage_key}] Error chunking {pdf_path} due to {e}\\n\")\n",
    "\n",
    "preprocess_and_chunk_pdfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Generation Step\n",
    "\n",
    "This is the main generation loop. It iterates through all files in `chunked_data` (both chunks and full documents), generates pairs for each, and saves the combined results into CSV files in the `generated_data` directory, grouped by the original filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_GENERATION_PROMPT = r\"\"\"\n",
    "You are an expert data scientist tasked with creating a high-quality dataset for instruction-tuning a large language model.\n",
    "\n",
    "Your primary goal is to generate {0} distinct and high-quality instruction-response pairs from the provided document.\n",
    "\n",
    "## Key Instructions\n",
    "1.  **Instruction (`input`):** Create a clear and specific prompt a user would ask. Vary the tasks: include direct questions, summarization requests, comparisons between concepts, and analytical prompts that require reasoning. Ensure a range of complexity.\n",
    "2.  **Response (`output`):** Write a detailed, accurate, and direct answer as if you are an expert on the subject. The response must fully and exclusively satisfy the user's instruction, and respond in a complete and factual manner.\n",
    "3.  **Source Grounding:** Base all responses **exclusively** on the information within the provided document.\n",
    "4.  **No Self-Reference:** Do not mention the source document in your responses. Avoid any phrases like \"According to the document...\" or \"The provided text states...\".\n",
    "\n",
    "## Formatting Requirements\n",
    "* Your entire output **must** be a single, valid JSON object. Do not include any text, explanations, or markdown formatting before or after the JSON structure.\n",
    "* **CRITICAL:** All strings within the JSON must be properly escaped to ensure the output is parsable.\n",
    "    * All backslashes (`\\`) must be escaped as (`\\\\`). For example, the text `\\x48` must be written in the JSON string as `\\\\x48`, and `\\n` must be written as `\\\\n`.\n",
    "    * All newlines (`\\n`) must be escaped as (`\\\\n`). For example, the text `hi\\n` must be written in the JSON string as `hi\\\\n`.\n",
    "    * All double quotes (`\"`) must be escaped as (`\\\"`). For example, the text `he said \"hello\"` must be written as `he said \\\"hello\\\"` and `\"` must be written as `\\\"`.\n",
    "    * Do not escape any other characters as those are invalid characters. For example, the text `this 'item'` must be written as `this 'item'` and `printf(\"%d\\n\", 5);` must be written as `printf(\\\"%d\\\\n\\\", 5);`.\n",
    "\n",
    "Use the following JSON structure:\n",
    "```json\n",
    "{{\n",
    "  \"results\": [\n",
    "    {{\n",
    "      \"input\": \"Your first generated input here.\",\n",
    "      \"output\": \"Your first generated output here.\"\n",
    "    }},\n",
    "    {{\n",
    "      \"input\": \"Your second generated input here.\",\n",
    "      \"output\": \"Your second generated output here.\"\n",
    "    }},\n",
    "    ... and so on for all {0} pairs. \n",
    "  ]\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_name(file_name):\n",
    "    return re.sub(r'\\.[^.]+$', '', os.path.basename(file_name))\n",
    "\n",
    "def generate_data_for_chunk(file_path,\n",
    "                            model=MODEL,\n",
    "                            model_id=MODEL_ID,\n",
    "                            dest_dir=GENERATED_DATA_DIR,\n",
    "                            data_generation_prompt=DATA_GENERATION_PROMPT,\n",
    "                            iterations=NUM_ITERATIONS,\n",
    "                            pairs_per_iteration=PAIRS_PER_ITERATION,\n",
    "                            retries=NUM_RETRIES,\n",
    "                            stage_key='DATA_GENERATION'):\n",
    "\n",
    "    full_file_name = os.path.basename(file_path)\n",
    "    base_file_name = get_base_name(full_file_name)\n",
    "    trgt_dir = os.path.join(dest_dir, base_file_name)\n",
    "    \n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)\n",
    "    if not os.path.exists(trgt_dir):\n",
    "        os.makedirs(trgt_dir)\n",
    "    \n",
    "    try:\n",
    "        uploaded_file = model.files.upload(file=file_path, config={'display_name': full_file_name})\n",
    "        full_df = pd.DataFrame()\n",
    "        print(f\"[{stage_key}] Successfully uploaded '{full_file_name}'\")\n",
    "        \n",
    "        for iteration_num in range(1, iterations + 1):\n",
    "            print(f\"[{stage_key}] Starting data generation ({iteration_num}/{iterations}) for '{full_file_name}'\")\n",
    "            for attempt in range(retries):\n",
    "                try:\n",
    "                    prompt = str.format(data_generation_prompt, pairs_per_iteration)\n",
    "                    response = model.models.generate_content(\n",
    "                        model=model_id,\n",
    "                        contents=[prompt, uploaded_file],\n",
    "                        config={\"temperature\": 0.7, \"top_p\": 0.95}\n",
    "                    )\n",
    "                    response_text = response.text[7:-3]\n",
    "                    results = json.loads(response_text)[\"results\"]\n",
    "                    print(f\"[{stage_key}] Received {len(results)} pairs in response ({iteration_num}/{iterations}) for '{full_file_name}'{' (attempt' + attempt + '/' + retries + ')' if attempt > 1 else ''}\")\n",
    "\n",
    "                    generated_pairs = [{\n",
    "                        \"input\": r\"{}\".format(result[\"input\"].strip()),\n",
    "                        \"output\": r\"{}\".format(result[\"output\"].strip())\n",
    "                    } for result in results if result.get('input') and result.get('output')]\n",
    "                    print(f\"[{stage_key}] Cumulative total of {len(generated_pairs)} pairs generated for '{full_file_name}' after generation ({iteration_num}/{iterations}){' (attempt' + attempt + '/' + retries + ')' if attempt > 1 else ''}\")\n",
    "\n",
    "                    df = pd.DataFrame(generated_pairs)\n",
    "                    pd.concat([full_df, df])\n",
    "                    df.to_csv(os.path.join(trgt_dir, f'{base_file_name}_{iteration_num}.csv'), index=False)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"[{stage_key}] Data generation ({iteration_num}/{iterations}){' (attempt' + attempt + '/' + retries + ')' if attempt > 1 else ''} for '{full_file_name}' failed due to {e}\")\n",
    "        return full_df\n",
    "    except Exception as e:\n",
    "        print(f\"[{stage_key}] Data generation for '{full_file_name}' failed due to {e}\")\n",
    "\n",
    "    finally:\n",
    "        if uploaded_file:\n",
    "            model.files.delete(name=uploaded_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_for_chunks(raw_dir=RAW_DATA_DIR,\n",
    "                             chunked_dir=CHUNKED_DATA_DIR,\n",
    "                             dest_dir=GENERATED_DATA_DIR,\n",
    "                             stage_key='DATA_GENERATION',\n",
    "                             max_workers=MAX_CONCURRENCY):\n",
    "    \n",
    "    original_pdf_files = glob.glob(os.path.join(raw_dir, \"*.pdf\"))\n",
    "\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)\n",
    "\n",
    "    print(f\"[{stage_key}] Found {len(original_pdf_files)} original documents to process for generation\\n\")\n",
    "\n",
    "    all_files_to_process = []\n",
    "    \n",
    "    for original_pdf_file in original_pdf_files:\n",
    "        base_name = get_base_name(original_pdf_file)\n",
    "        files_to_process = sorted(glob.glob(os.path.join(chunked_dir, f\"{base_name}*.pdf\")))\n",
    "\n",
    "        if not files_to_process:\n",
    "            print(f\"[{stage_key}] No files found for '{base_name}'\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"[{stage_key}] Generating data for '{base_name}'\")\n",
    "        all_files_to_process.extend(files_to_process)\n",
    "    \n",
    "    full_df = pd.DataFrame()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(generate_data_for_chunk, file_path): file_path \n",
    "            for file_path in all_files_to_process\n",
    "        }\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            file_path = futures[future]\n",
    "            pd.concat([full_df, future.result()])\n",
    "            print(f\"[{stage_key}] Cumulative total of {len(full_df)} pairs generated\")\n",
    "\n",
    "\n",
    "generate_data_for_chunks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_pdf_files = glob.glob(os.path.join(RAW_DATA_DIR, \"*.pdf\"))\n",
    "# original_basenames = [os.path.basename(f).replace('.pdf', '') for f in original_pdf_files]\n",
    "\n",
    "# print(f\"Found {len(original_basenames)} original documents to process for generation.\")\n",
    "\n",
    "# if not os.path.exists(GENERATED_DATA_DIR):\n",
    "#     os.makedirs(GENERATED_DATA_DIR)\n",
    "\n",
    "# for basename in original_basenames:\n",
    "#     print(f\"\\n--- Generating data for original file: {basename}.pdf ---\")\n",
    "#     all_pairs = []\n",
    "    \n",
    "#     files_to_process = sorted(glob.glob(os.path.join(CHUNKED_DATA_DIR, f\"{basename}*.pdf\")))\n",
    "\n",
    "#     if not files_to_process:\n",
    "#         print(f\"  - No files found for {basename}. Skipping.\")\n",
    "#         continue\n",
    "\n",
    "#     for i, file_path in enumerate(files_to_process):\n",
    "#         full_file_name = os.path.basename(file_path)\n",
    "#         only_file_name = re.sub(r'\\.[^.]+$', '', full_file_name)\n",
    "\n",
    "#         if not os.path.exists(GENERATED_DATA_DIR):\n",
    "#             os.makedirs(GENERATED_DATA_DIR)\n",
    "#         if not os.path.exists(f'{GENERATED_DATA_DIR}/{only_file_name}'):\n",
    "#             os.makedirs(f'{GENERATED_DATA_DIR}/{only_file_name}')\n",
    "\n",
    "#         print(f\"  - Processing file {i+1}/{len(files_to_process)}: {full_file_name}\")\n",
    "#         uploaded_file = None\n",
    "#         try:\n",
    "#             uploaded_file = MODEL.files.upload(file=file_path, config={'display_name': full_file_name})\n",
    "            \n",
    "#             # Loop for the number of API calls\n",
    "#             for iteration_num in range(1, NUM_ITERATIONS + 1):\n",
    "#                 retries = 3\n",
    "#                 for attempt in range(retries):  # Retry mechanism\n",
    "#                     error_data = None\n",
    "#                     try:\n",
    "#                         print(f\"    - Starting iteration {iteration_num}/{NUM_ITERATIONS} (requesting {PAIRS_PER_ITERATION} pairs)... \")\n",
    "                        \n",
    "#                         prompt = r\"\"\"\n",
    "# You are an expert data scientist tasked with creating a high-quality dataset for instruction-tuning a large language model.\n",
    "\n",
    "# Your primary goal is to generate {0} distinct and high-quality instruction-response pairs from the provided document.\n",
    "\n",
    "# ## Key Instructions\n",
    "# 1.  **Instruction (`input`):** Create a clear and specific prompt a user would ask. Vary the tasks: include direct questions, summarization requests, comparisons between concepts, and analytical prompts that require reasoning. Ensure a range of complexity.\n",
    "# 2.  **Response (`output`):** Write a detailed, accurate, and direct answer as if you are an expert on the subject. The response must fully and exclusively satisfy the user's instruction, and respond in a complete and factual manner.\n",
    "# 3.  **Source Grounding:** Base all responses **exclusively** on the information within the provided document.\n",
    "# 4.  **No Self-Reference:** Do not mention the source document in your responses. Avoid any phrases like \"According to the document...\" or \"The provided text states...\".\n",
    "\n",
    "# ## Formatting Requirements\n",
    "# * Your entire output **must** be a single, valid JSON object. Do not include any text, explanations, or markdown formatting before or after the JSON structure.\n",
    "# * **CRITICAL:** All strings within the JSON must be properly escaped to ensure the output is parsable.\n",
    "#     * All backslashes (`\\`) must be escaped as (`\\\\`). For example, the text `\\x48` must be written in the JSON string as `\\\\x48`, and `\\n` must be written as `\\\\n`.\n",
    "#     * All newlines (`\\n`) must be escaped as (`\\\\n`). For example, the text `hi\\n` must be written in the JSON string as `hi\\\\n`.\n",
    "#     * All double quotes (`\"`) must be escaped as (`\\\"`). For example, the text `he said \"hello\"` must be written as `he said \\\"hello\\\"` and `\"` must be written as `\\\"`.\n",
    "#     * Do not escape any other characters as those are invalid characters. For example, the text `this 'item'` must be written as `this 'item'` and `printf(\"%d\\n\", 5);` must be written as `printf(\\\"%d\\\\n\\\", 5);`.\n",
    "\n",
    "# Use the following JSON structure:\n",
    "# ```json\n",
    "# {{\n",
    "#   \"results\": [\n",
    "#     {{\n",
    "#       \"input\": \"Your first generated input here.\",\n",
    "#       \"output\": \"Your first generated output here.\"\n",
    "#     }},\n",
    "#     {{\n",
    "#       \"input\": \"Your second generated input here.\",\n",
    "#       \"output\": \"Your second generated output here.\"\n",
    "#     }},\n",
    "#     ... and so on for all {0} pairs. \n",
    "#   ]\n",
    "# }}\n",
    "# \"\"\"\n",
    "#                         prompt = str.format(prompt, PAIRS_PER_ITERATION)\n",
    "#                         response = MODEL.models.generate_content(\n",
    "#                             model=MODEL_ID,\n",
    "#                             contents=[prompt, uploaded_file],\n",
    "#                             config={\"temperature\": 0.7, \"top_p\": 0.95}\n",
    "#                         )\n",
    "#                         response_text = response.text[7:-3]\n",
    "\n",
    "#                         with open(f'{GENERATED_DATA_DIR}/{only_file_name}/{only_file_name}_{iteration_num}.txt', 'w') as f:\n",
    "#                             f.write(response_text)\n",
    "                        \n",
    "#                         error_data = response_text\n",
    "#                         results = json.loads(response_text)[\"results\"]\n",
    "#                         print(f\"      - Received {len(results)} pairs in response.\")\n",
    "\n",
    "#                         for result in results:\n",
    "#                             if result.get(\"input\") and result.get(\"output\"):\n",
    "#                                 all_pairs.append({\n",
    "#                                     \"input\": r\"{}\".format(result[\"input\"].strip()),\n",
    "#                                     \"output\": r\"{}\".format(result[\"output\"].strip())\n",
    "#                                 })\n",
    "#                             else:\n",
    "#                                 print(\"      Warning: Generated empty input or output for a pair.\")\n",
    "#                         print(f\"      - Total of {len(all_pairs)} pairs generated.\")\n",
    "#                         break\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"      An error occurred during iteration {iteration_num}: {e}\")\n",
    "#                         print(f\"      Error data: {error_data}\")\n",
    "#                         continue\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"      An error occurred during generation: {e}\")\n",
    "#         finally:\n",
    "#             if uploaded_file:\n",
    "#                 MODEL.files.delete(name=uploaded_file.name)\n",
    "    \n",
    "#     if all_pairs:\n",
    "#         output_filename = f\"{basename}_full.csv\"\n",
    "#         output_path = os.path.join(GENERATED_DATA_DIR, output_filename)\n",
    "#         df = pd.DataFrame(all_pairs)\n",
    "#         df.to_csv(output_path, index=False)\n",
    "#         print(f\"\\nSuccessfully generated a total of {len(all_pairs)} pairs for {basename}.pdf and saved to {output_path}\")\n",
    "#     else:\n",
    "#         print(f\"\\nNo data was generated for the file {basename}.pdf.\")\n",
    "\n",
    "# print(\"\\n--- All files processed. ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
